{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Package Dependency\n",
    "\n",
    "- [nltk](https://www.nltk.org)\n",
    "- [sklearn](http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ...\n",
      "40000, 40000\n",
      "[\"love the staff, love the meat, love the place. prepare for a long line around lunch or dinner hours. they ask you how you want you meat, lean or something maybe, i can't remember. just say you don't want it too fatty. get a half sour pickle and a hot pepper. hand cut french fries too.\", \"super simple place but amazing nonetheless. it's been around since the 30's and they still serve the same thing they started with: a bologna and salami sandwich with mustard. staff was very helpful and friendly.\"]\n",
      "['5', '5', '5', '5', '4']\n",
      "Development data ...\n",
      "5000, 5000\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Load data\n",
    "trn_texts = open(\"trn-reviews.txt\").read().strip().lower().split(\"\\n\")\n",
    "trn_labels = open(\"trn-labels.txt\").read().strip().lower().split(\"\\n\")\n",
    "print(\"Training data ...\")\n",
    "print(\"%d, %d\" % (len(trn_texts), len(trn_labels)))\n",
    "print(trn_texts[:2])\n",
    "print(trn_labels[:5])\n",
    "\n",
    "dev_texts = open(\"dev-reviews.txt\").read().strip().split(\"\\n\")\n",
    "dev_labels = open(\"dev-labels.txt\").read().strip().split(\"\\n\")\n",
    "print(\"Development data ...\")\n",
    "print(\"%d, %d\" % (len(dev_texts), len(dev_labels)))\n",
    "dev_tokens = WordPunctTokenizer().tokenize_sents(dev_texts)\n",
    "trn_tokens = WordPunctTokenizer().tokenize_sents(trn_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "glove6B = loadGloveModel('glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove6B['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Great', 'Great', 'Great', 'place', 'to', 'eat', '!', 'I', 'have', 'a', 'friend', 'in', 'the', 'casino', 'industry', 'that', 'always', 'gets', 'the', 'hook', 'up', 'here', '.', 'I', \"'\", 'm', 'not', 'a', 'seafood', 'person', 'but', 'its', 'unlimited', 'crab', 'legs', 'first', 'of', 'all', 'so', 'theres', 'that', '.', 'I', 'always', 'see', 'my', 'friends', 'grab', 'like', '20', '+', 'of', 'those', 'things', '.', 'I', 'usually', 'get', 'the', 'prime', 'rib', 'and', 'other', 'things', 'that', 'go', 'with', 'it', '.', 'Nothing', 'besides', 'the', 'prime', 'rib', ',', 'crab', 'legs', ',', 'and', 'lamb', 'really', 'stick', 'out', 'as', 'far', 'as', 'meat', '.', 'There', 'is', 'a', 'asian', 'food', 'section', ',', 'dessert', ',', 'and', 'from', 'what', 'I', 'can', 'tell', 'a', 'cross', 'between', 'greek', 'and', 'indian', 'section', '.', 'The', 'food', 'was', 'great', '.', 'You', 'can', 'order', 'booze', 'of', 'course', 'but', 'it', 'cost', 'extra', '.', 'This', 'is', 'the', 'pricing', ';', 'Breakfast', '$', '21', '.', '99Monday', '-', 'Friday', '|', '7', ':', '00', 'a', '.', 'm', '.', '-', '11', ':', '00', 'a', '.', 'm', '.', 'Lunch', '$', '25', '.', '99', 'Monday', '-', 'Friday', '|', '11', ':', '00', 'a', '.', 'm', '.', '-', '3', ':', '00', 'p', '.', 'm', '.', 'Dinner', '$', '36', '.', '99Monday', '-', 'Thursday', '|', '3', ':', '00', '-', '10', ':', '00p', '.', 'm', '.', 'Gourmet', 'Dinner', '$', '41', '.', '99Friday', '-', 'Sunday', '|', '3', ':', '00', '-', '10', ':', '00', 'p', '.', 'm', '.', 'Weekend', 'Brunch', '$', '31', '.', '99Unlimited', 'Champagne', 'Brunch', '$', '43', '.', '98Saturday', '&', 'Sunday', '|', '7', ':', '00', 'a', '.', 'm', '.', '-', '3', ':', '00', 'p', '.', 'm', '.', 'Chef', \"'\", 's', 'Table', 'Daily', 'Add', '$', '25Reservation', 'Required', '4', ':', '00', 'p', '.', 'm', '.', '-', '10', ':', '00', 'p', '.', 'm', '.', 'Holiday', 'Pricing', 'ChangesBrunch', '$', '35', '.', '99', '-', '$', '38', '.', '99', '|', '7', ':', '00', 'a', '.', 'm', '.', '-', '3', ':', '00', 'p', '.', 'm', '.', 'Brunch', 'w', '/', 'Champagne', '$', '49', '.', '98', '-', '$', '53', '.', '99Dinner', '$', '44', '.', '99', '-', '$', '51', '.', '99', '|', '3', ':', '00', 'p', '.', 'm', '.', '-', '10', ':', '00', 'p', '.', 'm', '.', 'Holiday', 'Feast', 'wil', 'start', 'at', '12', ':', '00', 'p', '.', 'm', '.', 'on', 'Thanksgiving', 'Day', '&', 'Christmas', 'DayAs', 'a', 'tip', 'I', 'would', 'get', 'there', 'during', 'lunch', 'pricing', 'and', 'stay', 'through', 'out', 'dinner', ':)', 'REMEMBER', 'EVERYONE', 'WANTS', 'TO', 'COME', 'TO', 'THIS', 'PLACE', 'SO', 'THE', 'LINES', 'CAN', 'GET', 'LONG', 'FOR', 'DINNER', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_sentence_vectors = []\n",
    "for tokens in trn_tokens:\n",
    "    sentenceVector = np.zeros(50)\n",
    "    tokenCount = 0\n",
    "    for token in token:\n",
    "        tokenVectors.append(glove6B[token])\n",
    "        tokenCount += 1\n",
    "    trn_sentence_vectors.append(np.array(to))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction\n",
    "\n",
    "Please refer to the document of [_CountVectorizer_](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for the parameters of this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 686)\n",
      "(5000, 686)\n"
     ]
    }
   ],
   "source": [
    "choice = 3\n",
    "\n",
    "if choice == 1:\n",
    "    print(\"Preprocessing without any feature selection\")\n",
    "    vectorizer = CountVectorizer(lowercase=False)\n",
    "    # vocab size 77166\n",
    "elif choice == 2:\n",
    "    print(\"Lowercasing all the tokens\")\n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    # vocab size 60610\n",
    "elif choice == 3:\n",
    "    vectorizer = CountVectorizer(lowercase=True, min_df=0.017, max_df=0.95)\n",
    "elif choice == 4:\n",
    "    vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 2), min_df=0.017, max_df=0.95)\n",
    "else: \n",
    "    raise ValueError(\"Unrecognized value: choice = %d\" % choice)\n",
    "\n",
    "trn_data = vectorizer.fit_transform(trn_texts)\n",
    "print(trn_data.shape)\n",
    "# print(trn_data[0])\n",
    "dev_data = vectorizer.transform(dev_texts)\n",
    "print(dev_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression\n",
    "\n",
    "Please refer to the document of [_LogisticRegression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for the parameters of this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.628750\n",
      "Dev accuracy = %f 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define a LR classifier\n",
    "classifier = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\", penalty='l1')\n",
    "classifier.fit(trn_data, trn_labels)\n",
    "\n",
    "# Measure the performance on training and dev data\n",
    "print(\"Training accuracy = %f\" % classifier.score(trn_data, trn_labels))\n",
    "print(\"Dev accuracy = %f\", classifier.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy log with dfferent parameters\n",
    "lower case, stop words\n",
    "- Training accuracy = 0.909475\n",
    "- Dev accuracy = %f 0.6014\n",
    "\n",
    "lower case\n",
    "- Training accuracy = 0.917175\n",
    "- Dev accuracy = %f 0.6146\n",
    "\n",
    "lowercase=True, stop_words='english', max_features=20000\n",
    "- Training accuracy = 0.879750\n",
    "- Dev accuracy = %f 0.5976\n",
    "\n",
    "lowercase=True, max_df=0.8\n",
    "- Training accuracy = 0.917475\n",
    "- Dev accuracy = %f 0.6136\n",
    "\n",
    "lowercase=True, max_df=0.7\n",
    "- Training accuracy = 0.917275\n",
    "- Dev accuracy = %f 0.6132\n",
    "\n",
    "lowercase=True, max_df=0.9\n",
    "- Training accuracy = 0.917225\n",
    "- Dev accuracy = %f 0.6148\n",
    "\n",
    "lowercase=True, max_df=0.95\n",
    "- Training accuracy = 0.917175\n",
    "- Dev accuracy = %f 0.6146\n",
    "\n",
    "lowercase=True, max_df=0.925\n",
    "- Training accuracy = 0.917175\n",
    "- Dev accuracy = %f 0.6146\n",
    "\n",
    "lowercase=True, max_df=0.875\n",
    "- Training accuracy = 0.917475\n",
    "- Dev accuracy = %f 0.6136\n",
    "\n",
    "lowercase=True, min_df=0.01\n",
    "- Training accuracy = 0.652525\n",
    "- Dev accuracy = %f 0.6242\n",
    "\n",
    "lowercase=True, min_df=0.02\n",
    "- Training accuracy = 0.623700\n",
    "- Dev accuracy = %f 0.6246\n",
    "\n",
    "lowercase=True, min_df=0.03\n",
    "- Training accuracy = 0.604925\n",
    "- Dev accuracy = %f 0.6104\n",
    "\n",
    "lowercase=True, min_df=0.015\n",
    "- Training accuracy = 0.633650\n",
    "- Dev accuracy = %f 0.6264\n",
    "\n",
    "lowercase=True, min_df=0.0175\n",
    "- Training accuracy = 0.628725\n",
    "- Dev accuracy = %f 0.6294\n",
    "\n",
    "lowercase=True, min_df=0.016\n",
    "- Training accuracy = 0.631175\n",
    "- Dev accuracy = %f 0.6252\n",
    "\n",
    "lowercase=True, min_df=0.017\n",
    "- Training accuracy = 0.629000\n",
    "- Dev accuracy = %f 0.6294\n",
    "\n",
    "lowercase=True, min_df=0.018\n",
    "- Training accuracy = 0.628525\n",
    "- Dev accuracy = %f 0.6286\n",
    "\n",
    "lowercase=True, min_df=0.017, max_df=0.95\n",
    "- Training accuracy = 0.629000\n",
    "- Dev accuracy = %f 0.6294"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
